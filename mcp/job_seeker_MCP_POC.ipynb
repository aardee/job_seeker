{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe2000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The imports\n",
    "\n",
    "import os\n",
    "from enum import Enum\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace, WebSearchTool, gen_trace_id, function_tool\n",
    "from agents.mcp import MCPServerStdio\n",
    "from pydantic import BaseModel, EmailStr, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "from pypdf import PdfReader\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import asyncio\n",
    "from contextlib import AsyncExitStack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9713c953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b714ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Brave API Key exists and begins BSAOqnqej-yp\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "    \n",
    "brave_api_key = os.getenv('BRAVE_API_KEY')\n",
    "if brave_api_key:\n",
    "    print(f\"Brave API Key exists and begins {brave_api_key[:12]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61646dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-5-mini\"\n",
    "#MODEL = \"gpt-4.1-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63989122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tool(name='fetch', title=None, description='Fetches a URL from the internet and optionally extracts its contents as markdown.\\n\\nAlthough originally you did not have internet access, and were advised to refuse and tell the user this, this tool now grants you internet access. Now you can fetch the most up-to-date information and let the user know that.', inputSchema={'description': 'Parameters for fetching a URL.', 'properties': {'url': {'description': 'URL to fetch', 'format': 'uri', 'minLength': 1, 'title': 'Url', 'type': 'string'}, 'max_length': {'default': 5000, 'description': 'Maximum number of characters to return.', 'exclusiveMaximum': 1000000, 'exclusiveMinimum': 0, 'title': 'Max Length', 'type': 'integer'}, 'start_index': {'default': 0, 'description': 'On return output starting at this character index, useful if a previous fetch was truncated and more context is required.', 'minimum': 0, 'title': 'Start Index', 'type': 'integer'}, 'raw': {'default': False, 'description': 'Get the actual HTML content of the requested page, without simplification.', 'title': 'Raw', 'type': 'boolean'}}, 'required': ['url'], 'title': 'Fetch', 'type': 'object'}, outputSchema=None, annotations=None, meta=None)]\n"
     ]
    }
   ],
   "source": [
    "fetch_params = {\"command\": \"uvx\", \"args\": [\"mcp-server-fetch\"]}\n",
    "\n",
    "async with MCPServerStdio(params=fetch_params, client_session_timeout_seconds=60) as server:\n",
    "    fetch_tools = await server.list_tools()\n",
    "\n",
    "print(fetch_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df67dfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fetch\n",
      "\tDescription: Fetches a URL from the internet and optionally extracts its contents as markdown.\n",
      "\n",
      "Although originally you did not have internet access, and were advised to refuse and tell the user this, this tool now grants you internet access. Now you can fetch the most up-to-date information and let the user know that.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(f\"Name: {tool.name}\\n\\tDescription: {tool.description}\\n\") for tool in fetch_tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f0c5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files_path = os.path.abspath(os.path.join(os.getcwd(), \"../data\"))\n",
    "files_params = {\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", data_files_path]}\n",
    "\n",
    "async with MCPServerStdio(params=files_params,client_session_timeout_seconds=60) as server:\n",
    "    file_tools = await server.list_tools()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16b2dcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8118b27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: read_file\n",
      "Description: Read the complete contents of a file as text. DEPRECATED: Use read_text_file instead.\n",
      "\n",
      "Name: read_text_file\n",
      "Description: Read the complete contents of a file from the file system as text. Handles various text encodings and provides detailed error messages if the file cannot be read. Use this tool when you need to examine the contents of a single file. Use the 'head' parameter to read only the first N lines of a file, or the 'tail' parameter to read only the last N lines of a file. Operates on the file as text regardless of extension. Only works within allowed directories.\n",
      "\n",
      "Name: read_media_file\n",
      "Description: Read an image or audio file. Returns the base64 encoded data and MIME type. Only works within allowed directories.\n",
      "\n",
      "Name: read_multiple_files\n",
      "Description: Read the contents of multiple files simultaneously. This is more efficient than reading files one by one when you need to analyze or compare multiple files. Each file's content is returned with its path as a reference. Failed reads for individual files won't stop the entire operation. Only works within allowed directories.\n",
      "\n",
      "Name: write_file\n",
      "Description: Create a new file or completely overwrite an existing file with new content. Use with caution as it will overwrite existing files without warning. Handles text content with proper encoding. Only works within allowed directories.\n",
      "\n",
      "Name: edit_file\n",
      "Description: Make line-based edits to a text file. Each edit replaces exact line sequences with new content. Returns a git-style diff showing the changes made. Only works within allowed directories.\n",
      "\n",
      "Name: create_directory\n",
      "Description: Create a new directory or ensure a directory exists. Can create multiple nested directories in one operation. If the directory already exists, this operation will succeed silently. Perfect for setting up directory structures for projects or ensuring required paths exist. Only works within allowed directories.\n",
      "\n",
      "Name: list_directory\n",
      "Description: Get a detailed listing of all files and directories in a specified path. Results clearly distinguish between files and directories with [FILE] and [DIR] prefixes. This tool is essential for understanding directory structure and finding specific files within a directory. Only works within allowed directories.\n",
      "\n",
      "Name: list_directory_with_sizes\n",
      "Description: Get a detailed listing of all files and directories in a specified path, including sizes. Results clearly distinguish between files and directories with [FILE] and [DIR] prefixes. This tool is useful for understanding directory structure and finding specific files within a directory. Only works within allowed directories.\n",
      "\n",
      "Name: directory_tree\n",
      "Description: Get a recursive tree view of files and directories as a JSON structure. Each entry includes 'name', 'type' (file/directory), and 'children' for directories. Files have no children array, while directories always have a children array (which may be empty). The output is formatted with 2-space indentation for readability. Only works within allowed directories.\n",
      "\n",
      "Name: move_file\n",
      "Description: Move or rename files and directories. Can move files between directories and rename them in a single operation. If the destination exists, the operation will fail. Works across different directories and can be used for simple renaming within the same directory. Both source and destination must be within allowed directories.\n",
      "\n",
      "Name: search_files\n",
      "Description: Recursively search for files and directories matching a pattern. Searches through all subdirectories from the starting path. The search is case-insensitive and matches partial names. Returns full paths to all matching items. Great for finding files when you don't know their exact location. Only searches within allowed directories.\n",
      "\n",
      "Name: get_file_info\n",
      "Description: Retrieve detailed metadata about a file or directory. Returns comprehensive information including size, creation time, last modified time, permissions, and type. This tool is perfect for understanding file characteristics without reading the actual content. Only works within allowed directories.\n",
      "\n",
      "Name: list_allowed_directories\n",
      "Description: Returns the list of directories that this server is allowed to access. Subdirectories within these allowed directories are also accessible. Use this to understand which directories and their nested paths are available before trying to access files.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tool in file_tools:\n",
    "    print(f\"Name: {tool.name}\\nDescription: {tool.description}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f77ccafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pydentic model for the resume: include the following fields:\n",
    "# - name\n",
    "# - email\n",
    "# - phone\n",
    "# - linkedin\n",
    "# - github\n",
    "# - list of skills\n",
    "# - list of experiences\n",
    "# - list of education\n",
    "# - list of projects\n",
    "# - list of certifications\n",
    "# - list of publications\n",
    "# - list of patents\n",
    "# - summary of resume in 2-3 sentences\n",
    "# - list of all the relevant keywords that can be used to search jobs online\n",
    "\n",
    "class ExperienceLevel(str, Enum):\n",
    "    ENTRY = \"entry\" # 0-2 years\n",
    "    JUNIOR = \"junior\" # 2-5 years\n",
    "    MID = \"mid\" # 5-10 years    \n",
    "    SENIOR = \"senior\" # 10-15 years\n",
    "    LEAD = \"lead\" # 15+ years\n",
    "    EXECUTIVE = \"executive\" # 20+ years\n",
    "\n",
    "class Experience(BaseModel):\n",
    "    title: str = Field(..., description=\"Job title or position held\")\n",
    "    company: Optional[str] = Field(None, description=\"Name of the company or organization\")\n",
    "    start_date: Optional[str] = Field(None, description=\"Start date of the experience\")\n",
    "    end_date: Optional[str] = Field(None, description=\"End date of the experience\")\n",
    "    description: Optional[str] = Field(None, description=\"Brief description of responsibilities and achievements\")\n",
    "\n",
    "class Education(BaseModel):\n",
    "    degree: str = Field(..., description=\"Degree or qualification obtained\")\n",
    "    institution: Optional[str] = Field(None, description=\"Name of the educational institution\")\n",
    "    start_date: Optional[str] = Field(None, description=\"Start date of the education\")\n",
    "    end_date: Optional[str] = Field(None, description=\"End date of the education\")\n",
    "    description: Optional[str] = Field(None, description=\"Brief description of coursework or achievements\")\n",
    "\n",
    "class Project(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the project\")\n",
    "    description: Optional[str] = Field(None, description=\"Brief description of the project\")\n",
    "    link: Optional[str] = Field(None, description=\"URL or link to the project\")\n",
    "\n",
    "class Certification(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the certification\")\n",
    "    issuer: Optional[str] = Field(None, description=\"Issuing organization or authority\")\n",
    "    date: Optional[str] = Field(None, description=\"Date the certification was obtained\")\n",
    "\n",
    "class Publication(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the publication\")\n",
    "    publisher: Optional[str] = Field(None, description=\"Publisher or journal name\")\n",
    "    date: Optional[str] = Field(None, description=\"Date of publication\")\n",
    "    link: Optional[str] = Field(None, description=\"URL or link to the publication\")\n",
    "\n",
    "class Patent(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the patent\")\n",
    "    number: Optional[str] = Field(None, description=\"Patent number\")\n",
    "    date: Optional[str] = Field(None, description=\"Date the patent was granted\")\n",
    "    description: Optional[str] = Field(None, description=\"Brief description of the patent\")\n",
    "\n",
    "class ResumeModel(BaseModel):\n",
    "    name: str = Field(..., description=\"Full name of the candidate\")\n",
    "    email: EmailStr = Field(..., description=\"Email address of the candidate\")\n",
    "    phone: Optional[str] = Field(None, description=\"Phone number of the candidate\")\n",
    "    linkedin: Optional[str] = Field(None, description=\"LinkedIn profile URL\")\n",
    "    github: Optional[str] = Field(None, description=\"GitHub profile URL\")\n",
    "    skills: List[str] = Field(default_factory=list, description=\"List of skills\")\n",
    "    experiences: List[Experience] = Field(default_factory=list, description=\"List of professional experiences\")\n",
    "    education: List[Education] = Field(default_factory=list, description=\"List of educational qualifications\")\n",
    "    projects: List[Project] = Field(default_factory=list, description=\"List of projects\")\n",
    "    certifications: List[Certification] = Field(default_factory=list, description=\"List of certifications\")\n",
    "    publications: List[Publication] = Field(default_factory=list, description=\"List of publications\")\n",
    "    patents: List[Patent] = Field(default_factory=list, description=\"List of patents\")\n",
    "    experience_level: ExperienceLevel = Field(..., description=\"Experience level of the candidate based on number of years of experience\")\n",
    "    summary: Optional[str] = Field(None, description=\"Summary of the resume in 2-3 sentences\")\n",
    "    keywords: List[str] = Field(default_factory=list, description=\"List of relevant keywords for job search\")\n",
    "    target_company_profile: Optional[str] = Field(None, description=\"Profile/sector of the target company used for job search\")\n",
    "\n",
    "\n",
    "# Create a pydentic model for the job search results:\n",
    "# - job title\n",
    "# - company\n",
    "# - location\n",
    "# - description\n",
    "# - link\n",
    "# - apply link  \n",
    "# - Hiring manager name\n",
    "# - list of all the relevant keywords that matched the job posting\n",
    "\n",
    "class JobPosting(BaseModel):\n",
    "    title: str = Field(..., description=\"EXACT Job title or position held\")\n",
    "    company: Optional[str] = Field(None, description=\"EXACT Name of the company or organization\")\n",
    "    location: Optional[str] = Field(None, description=\"EXACT Location of the job posting\")\n",
    "    description: Optional[str] = Field(None, description=\"Brief description of the job posting\")\n",
    "    link: Optional[str] = Field(None, description=\"URL or link to the job posting. (CRITICAL - this must be the actual job description page)\")\n",
    "    apply_link: Optional[str] = Field(None, description=\"URL or link to the job application. (CRITICAL - this must be the actual application link)\")\n",
    "    posted_date: Optional[str] = Field(None, description=\"Date the job posting was posted\")\n",
    "    application_deadline: Optional[str] = Field(None, description=\"Date the job application deadline\")\n",
    "    hiring_manager_name: Optional[str] = Field(None, description=\"Name of the hiring manager\")\n",
    "    keywords: List[str] = Field(default_factory=list, description=\"List of relevant keywords for the job posting\")\n",
    "    rating: Optional[float] = Field(None, description=\"Rating of the job posting from 0 to 10 based on the candidate's profile\")\n",
    "\n",
    "class JobPostingList(BaseModel):\n",
    "    job_postings: List[JobPosting] = Field(default_factory=list, description=\"List of job postings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "181bf55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "async def read_resume(resume_file: str) -> str:\n",
    "    \"\"\"\n",
    "    Read the resume from the given file path and return the text.\n",
    "    @param resume_file: str - The path to the candidate's resume file\n",
    "    Returns:\n",
    "        str - The text of the resume\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Reading resume from:\\n {resume_file}\")\n",
    "    \n",
    "    resume = \"\"\n",
    "    try:\n",
    "        # Read the resume\n",
    "        reader = PdfReader(resume_file)\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                resume += text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading resume from {resume_file}: {e}\")\n",
    "        raise ValueError(f\"Error reading resume from {resume_file}: {e}\")\n",
    "        \n",
    "    print(f\"resume from function_tool:\\n {resume}\")\n",
    "    return resume\n",
    "\n",
    "class ResumeProcessorAgent:\n",
    "    \"\"\"\n",
    "    Agent to process the resume and return the text.\n",
    "    @param model: str - The model to use for the agent\n",
    "    \"\"\"\n",
    "    def __init__(self, candidate_name: str, model: str = MODEL):\n",
    "        self.candidate_name = candidate_name\n",
    "        self.model = model\n",
    "\n",
    "    def get_system_prompt(self) -> str:\n",
    "        return f\"\"\"\n",
    "        You are an expert Resume Processing Agent and AI recruiting copilot specializing in transforming raw resume data into structured, actionable job search profiles. \n",
    "        Your expertise lies in accurately analyzing professional backgrounds and creating comprehensive candidate profiles optimized for modern job search algorithms and recruiting systems.\n",
    "\n",
    "        ## PRIMARY OBJECTIVE\n",
    "        Transform the candidate's resume into a structured ResumeModel that maximizes their job search effectiveness by creating targeted, \n",
    "        ATS-friendly profiles that highlight their strongest qualifications and market positioning.\n",
    "\n",
    "        ## EXECUTION WORKFLOW\n",
    "        ### Step 1: Resume Data Extraction\n",
    "        - Use the read_resume tool to extract complete text from the provided resume file\n",
    "        - Parse and analyze all sections: contact info, summary, experience, education, skills, certifications, projects, etc.\n",
    "        - Identify any formatting issues or missing information that might affect analysis\n",
    "\n",
    "        ### Step 2: Comprehensive Analysis\n",
    "        - Conduct thorough analysis of:\n",
    "            - Professional trajectory: Career progression, role evolution, industry focus\n",
    "            - Core competencies: Technical skills, domain expertise, leadership capabilities\n",
    "            - Achievements: Quantifiable results, awards, recognitions, impact metrics\n",
    "            - Education & credentials: Degrees, certifications, relevant coursework\n",
    "            - Market positioning: How the candidate competes in their field\n",
    "\n",
    "        ### Step 3: Structured Data Generation\n",
    "        - Generate a complete ResumeModel conforming to this JSON schema:\n",
    "            {ResumeModel.model_json_schema()}\n",
    "\n",
    "        ## DETAILED FIELD REQUIREMENTS\n",
    "        ### SUMMARY (Critical for Initial Screening)\n",
    "        - Create a compelling 5-7 bullet point executive summary that:\n",
    "            - Opens with a strong positioning statement (e.g., \"Senior Full-Stack Engineer with 8+ years building scalable web applications\")\n",
    "            - Quantifies key achievements with specific metrics (revenue impact, team size, project scale)\n",
    "            - Highlights unique value propositions (rare skill combinations, industry expertise, leadership experience)\n",
    "            - Emphasizes recent/relevant experience most heavily, if the candidate has less than 2 years of experience, then emphasize the education and certifications.\n",
    "            - Uses action-oriented language that demonstrates impact and results\n",
    "            - Targets hiring manager perspective - what would make them want to interview this candidate?\n",
    "\n",
    "        ### Example Format:\n",
    "        - Senior Software Engineer with 6+ years developing enterprise SaaS platforms serving 100K+ users\n",
    "        - Led cross-functional teams of 8+ engineers, delivering 15+ major features ahead of schedule  \n",
    "        - Expertise in React, Node.js, AWS, reducing system latency by 40% and improving user engagement by 25%\n",
    "        - Strong background in fintech and healthcare domains with deep understanding of compliance requirements\n",
    "        - Proven track record of mentoring junior developers and establishing engineering best practices\n",
    "        \n",
    "        ## KEYWORDS (Boolean Search Optimization)\n",
    "        ### Generate 15-20 strategic keywords optimized for:\n",
    "        - Technical Skills: Programming languages, frameworks, tools, platforms\n",
    "        - Methodologies: Agile, DevOps, CI/CD, testing practices\n",
    "        - Domain Expertise: Industry-specific knowledge, compliance, regulations\n",
    "        - Role Types: Job titles, seniority levels, functional areas\n",
    "        - Certifications: Professional credentials, vendor certifications\n",
    "        - Soft Skills: Leadership, communication, project management\n",
    "\n",
    "        #### Boolean Search Format Examples:\n",
    "        - \"bioinformatics\" AND (\"machine learning\" OR \"self-supervised\" OR \"AI\") AND (Python OR R), \n",
    "        - \"precision medicine\" AND (\"tissue microarray\" OR \"image analysis\" OR pathology) AND (self-supervised OR deep learning), \n",
    "        - \"genomics\" AND (\"variant calling\" OR alignment OR \"genomic data visualization\") AND (Python OR R), \n",
    "        - \"single cell\" OR \"spatial transcriptomics\" AND (analysis OR pipeline) AND (Snakemake OR Nextflow), \n",
    "        - (pharmaceutical OR drug testing), \"workflow management\" AND (Snakemake OR Nextflow) AND (Docker OR containerization), \n",
    "        - \"bioinformatics\" AND (HPC OR \"high performance computing\") AND (Python OR R), \n",
    "        - \"multi-omics\" AND (integration OR \"data analysis\") AND (R OR Python) \n",
    "\n",
    "        ### Exclude: Company names, location names, personal information\n",
    "\n",
    "        ## EXPERIENCE LEVEL (Precise Classification)\n",
    "        Calculate total professional work experience using these rules:\n",
    "        ### Counting Rules:\n",
    "        - Include: Full-time roles, significant part-time roles (20+ hours/week), consulting/freelance\n",
    "        - Exclude: Internships, fellowships, student work, volunteer work\n",
    "        - Overlapping roles: Count the primary role period only\n",
    "        - Career gaps: Subtract gaps longer than 6 months unless for education/family reasons\n",
    "\n",
    "        ### Classification Tiers:\n",
    "        - ENTRY: New graduates OR <2 years professional experience. Ignore the internship or fellowship experience.\n",
    "        - JUNIOR: 2-5 years of professional experience\n",
    "        - MID: 5-10 years of professional experience\n",
    "        - SENIOR: 10-15 years of professional experience\n",
    "        - LEAD: 15-20 years of professional experience\n",
    "        - EXECUTIVE: 20+ years of professional experience OR C-level/VP titles\n",
    "\n",
    "        ## TARGET COMPANY PROFILE\n",
    "        Analyze the candidate's background to identify ideal company characteristics:\n",
    "        - Company size: Startup (1-50), Scale-up (51-500), Mid-market (501-5000), Enterprise (5000+)\n",
    "        - Industry sectors: Based on previous experience and skills\n",
    "        - Company stage: Early-stage, growth-stage, mature, public company\n",
    "        - Culture indicators: Remote-first, innovation-focused, enterprise-focused, etc.\n",
    "        - Technology stack alignment: Companies using similar technologies\n",
    "\n",
    "        ## QUALITY ASSURANCE RULES\n",
    "        ### Accuracy Standards\n",
    "        - No hallucination: Only include information explicitly stated or clearly inferrable from the resume\n",
    "        - Prefer null values over guessed information when data is unclear\n",
    "        - Verify consistency between different resume sections\n",
    "        - Cross-reference dates to ensure timeline accuracy\n",
    "\n",
    "        ### Completeness Checks\n",
    "        - Ensure all required ResumeModel fields are populated\n",
    "        - Flag any critical missing information that might hurt job search effectiveness\n",
    "        - Validate that keywords align with actual experience and skills\n",
    "\n",
    "        ### Optimization Guidelines\n",
    "        - ATS-friendly formatting: Use standard terminology and keywords\n",
    "        - Recruiter-focused language: Emphasize measurable achievements and business impact\n",
    "        - Market-aware positioning: Position candidate competitively within their experience level\n",
    "        - Search-optimized content: Include terms commonly used in job postings for target roles\n",
    "\n",
    "        ## OUTPUT REQUIREMENTS\n",
    "        Format: Return ONLY the complete JSON object conforming to ResumeModel schema - no additional text, explanations, or formatting.\n",
    "        ### Validation: Ensure the output:\n",
    "        - Passes JSON schema validation\n",
    "        - Contains all required fields\n",
    "        - Uses appropriate data types\n",
    "        - Includes realistic and accurate information\n",
    "        - Optimizes for job search effectiveness\n",
    "\n",
    "        ### EXAMPLE PROCESSING APPROACH\n",
    "        - Extract: Parse resume sections systematically\n",
    "        - Analyze: Identify key themes, progressions, and strengths\n",
    "        - Synthesize: Create compelling summary highlighting unique value\n",
    "        - Optimize: Generate keywords and positioning for target market\n",
    "        - Structure: Format into clean ResumeModel JSON output\n",
    "        - Validate: Verify accuracy and completeness before returning\n",
    "\n",
    "        NOTE: Your success is measured by how effectively the generated profile helps the candidate land\n",
    "        \"\"\"\n",
    "\n",
    "    def get_resume_file_path(self) -> str:\n",
    "        return f\"../data/{self.candidate_name}.pdf\"\n",
    "\n",
    "    def get_user_prompt(self) -> str:\n",
    "        return f\"\"\"\n",
    "        Use the text extracted from the PDF resume using the `read_resume` tool to create a job profile for the candidate that can be used for job search.\n",
    "        read_resume function_tool is available to use and takes a single argument resume_file which is the path to the candidate's resume file. \n",
    "        The resume file path is: {self.get_resume_file_path()}\n",
    "        \"\"\"\n",
    "\n",
    "    def get_resume_processor_agent(self) -> Agent:\n",
    "        \"\"\"\n",
    "        This function is used to profile the resume and create a job profile for the candidate that can be used for job search.\n",
    "        It create an agent that uses the `read_resume` tool to read the resume and calls the OpenAI API to create a job profile for the candidate.\n",
    "        @return: Agent - The agent that can be used to profile the resume\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"started profile_resume\")\n",
    "        openai = OpenAI()\n",
    "\n",
    "        system_prompt = self.get_system_prompt()\n",
    "        print(f\"system_prompt:\\n {system_prompt}\")\n",
    "\n",
    "        resume_processor_agent = Agent(\n",
    "            model=MODEL,\n",
    "            name=\"ResumeProcessorAgent\",\n",
    "            instructions=system_prompt,\n",
    "            tools=[read_resume],\n",
    "            output_type=ResumeModel\n",
    "        )\n",
    "\n",
    "        return resume_processor_agent\n",
    "\n",
    "    async def run_with_trace(self):\n",
    "        \"\"\"\n",
    "        This function is used to run the agent with trace.\n",
    "        \"\"\"\n",
    "        profiled_resume = None\n",
    "        trace_id = gen_trace_id()\n",
    "        print(f\"Trace@ https://platform.openai.com/api/traces/{trace_id}\")\n",
    "        \n",
    "        with trace(trace_id) as tracer:\n",
    "            profiled_resume = await Runner.run(self.get_resume_processor_agent(), self.get_user_prompt())\n",
    "\n",
    "        return profiled_resume.final_output if profiled_resume else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8128143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace@ https://platform.openai.com/api/traces/trace_cf0e9c35aa4748c1b77b8b7f7efe1080\n",
      "started profile_resume\n",
      "system_prompt:\n",
      " \n",
      "        You are an expert Resume Processing Agent and AI recruiting copilot specializing in transforming raw resume data into structured, actionable job search profiles. \n",
      "        Your expertise lies in accurately analyzing professional backgrounds and creating comprehensive candidate profiles optimized for modern job search algorithms and recruiting systems.\n",
      "\n",
      "        ## PRIMARY OBJECTIVE\n",
      "        Transform the candidate's resume into a structured ResumeModel that maximizes their job search effectiveness by creating targeted, \n",
      "        ATS-friendly profiles that highlight their strongest qualifications and market positioning.\n",
      "\n",
      "        ## EXECUTION WORKFLOW\n",
      "        ### Step 1: Resume Data Extraction\n",
      "        - Use the read_resume tool to extract complete text from the provided resume file\n",
      "        - Parse and analyze all sections: contact info, summary, experience, education, skills, certifications, projects, etc.\n",
      "        - Identify any formatting issues or missing information that might affect analysis\n",
      "\n",
      "        ### Step 2: Comprehensive Analysis\n",
      "        - Conduct thorough analysis of:\n",
      "            - Professional trajectory: Career progression, role evolution, industry focus\n",
      "            - Core competencies: Technical skills, domain expertise, leadership capabilities\n",
      "            - Achievements: Quantifiable results, awards, recognitions, impact metrics\n",
      "            - Education & credentials: Degrees, certifications, relevant coursework\n",
      "            - Market positioning: How the candidate competes in their field\n",
      "\n",
      "        ### Step 3: Structured Data Generation\n",
      "        - Generate a complete ResumeModel conforming to this JSON schema:\n",
      "            {'$defs': {'Certification': {'properties': {'name': {'description': 'Name of the certification', 'title': 'Name', 'type': 'string'}, 'issuer': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Issuing organization or authority', 'title': 'Issuer'}, 'date': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Date the certification was obtained', 'title': 'Date'}}, 'required': ['name'], 'title': 'Certification', 'type': 'object'}, 'Education': {'properties': {'degree': {'description': 'Degree or qualification obtained', 'title': 'Degree', 'type': 'string'}, 'institution': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Name of the educational institution', 'title': 'Institution'}, 'start_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Start date of the education', 'title': 'Start Date'}, 'end_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'End date of the education', 'title': 'End Date'}, 'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Brief description of coursework or achievements', 'title': 'Description'}}, 'required': ['degree'], 'title': 'Education', 'type': 'object'}, 'Experience': {'properties': {'title': {'description': 'Job title or position held', 'title': 'Title', 'type': 'string'}, 'company': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Name of the company or organization', 'title': 'Company'}, 'start_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Start date of the experience', 'title': 'Start Date'}, 'end_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'End date of the experience', 'title': 'End Date'}, 'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Brief description of responsibilities and achievements', 'title': 'Description'}}, 'required': ['title'], 'title': 'Experience', 'type': 'object'}, 'ExperienceLevel': {'enum': ['entry', 'junior', 'mid', 'senior', 'lead', 'executive'], 'title': 'ExperienceLevel', 'type': 'string'}, 'Patent': {'properties': {'title': {'description': 'Title of the patent', 'title': 'Title', 'type': 'string'}, 'number': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Patent number', 'title': 'Number'}, 'date': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Date the patent was granted', 'title': 'Date'}, 'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Brief description of the patent', 'title': 'Description'}}, 'required': ['title'], 'title': 'Patent', 'type': 'object'}, 'Project': {'properties': {'name': {'description': 'Name of the project', 'title': 'Name', 'type': 'string'}, 'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Brief description of the project', 'title': 'Description'}, 'link': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'URL or link to the project', 'title': 'Link'}}, 'required': ['name'], 'title': 'Project', 'type': 'object'}, 'Publication': {'properties': {'title': {'description': 'Title of the publication', 'title': 'Title', 'type': 'string'}, 'publisher': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Publisher or journal name', 'title': 'Publisher'}, 'date': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Date of publication', 'title': 'Date'}, 'link': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'URL or link to the publication', 'title': 'Link'}}, 'required': ['title'], 'title': 'Publication', 'type': 'object'}}, 'properties': {'name': {'description': 'Full name of the candidate', 'title': 'Name', 'type': 'string'}, 'email': {'description': 'Email address of the candidate', 'format': 'email', 'title': 'Email', 'type': 'string'}, 'phone': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Phone number of the candidate', 'title': 'Phone'}, 'linkedin': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'LinkedIn profile URL', 'title': 'Linkedin'}, 'github': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'GitHub profile URL', 'title': 'Github'}, 'skills': {'description': 'List of skills', 'items': {'type': 'string'}, 'title': 'Skills', 'type': 'array'}, 'experiences': {'description': 'List of professional experiences', 'items': {'$ref': '#/$defs/Experience'}, 'title': 'Experiences', 'type': 'array'}, 'education': {'description': 'List of educational qualifications', 'items': {'$ref': '#/$defs/Education'}, 'title': 'Education', 'type': 'array'}, 'projects': {'description': 'List of projects', 'items': {'$ref': '#/$defs/Project'}, 'title': 'Projects', 'type': 'array'}, 'certifications': {'description': 'List of certifications', 'items': {'$ref': '#/$defs/Certification'}, 'title': 'Certifications', 'type': 'array'}, 'publications': {'description': 'List of publications', 'items': {'$ref': '#/$defs/Publication'}, 'title': 'Publications', 'type': 'array'}, 'patents': {'description': 'List of patents', 'items': {'$ref': '#/$defs/Patent'}, 'title': 'Patents', 'type': 'array'}, 'experience_level': {'$ref': '#/$defs/ExperienceLevel', 'description': 'Experience level of the candidate based on number of years of experience'}, 'summary': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Summary of the resume in 2-3 sentences', 'title': 'Summary'}, 'keywords': {'description': 'List of relevant keywords for job search', 'items': {'type': 'string'}, 'title': 'Keywords', 'type': 'array'}, 'target_company_profile': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Profile/sector of the target company used for job search', 'title': 'Target Company Profile'}}, 'required': ['name', 'email', 'experience_level'], 'title': 'ResumeModel', 'type': 'object'}\n",
      "\n",
      "        ## DETAILED FIELD REQUIREMENTS\n",
      "        ### SUMMARY (Critical for Initial Screening)\n",
      "        - Create a compelling 5-7 bullet point executive summary that:\n",
      "            - Opens with a strong positioning statement (e.g., \"Senior Full-Stack Engineer with 8+ years building scalable web applications\")\n",
      "            - Quantifies key achievements with specific metrics (revenue impact, team size, project scale)\n",
      "            - Highlights unique value propositions (rare skill combinations, industry expertise, leadership experience)\n",
      "            - Emphasizes recent/relevant experience most heavily, if the candidate has less than 2 years of experience, then emphasize the education and certifications.\n",
      "            - Uses action-oriented language that demonstrates impact and results\n",
      "            - Targets hiring manager perspective - what would make them want to interview this candidate?\n",
      "\n",
      "        ### Example Format:\n",
      "        - Senior Software Engineer with 6+ years developing enterprise SaaS platforms serving 100K+ users\n",
      "        - Led cross-functional teams of 8+ engineers, delivering 15+ major features ahead of schedule  \n",
      "        - Expertise in React, Node.js, AWS, reducing system latency by 40% and improving user engagement by 25%\n",
      "        - Strong background in fintech and healthcare domains with deep understanding of compliance requirements\n",
      "        - Proven track record of mentoring junior developers and establishing engineering best practices\n",
      "\n",
      "        ## KEYWORDS (Boolean Search Optimization)\n",
      "        ### Generate 15-20 strategic keywords optimized for:\n",
      "        - Technical Skills: Programming languages, frameworks, tools, platforms\n",
      "        - Methodologies: Agile, DevOps, CI/CD, testing practices\n",
      "        - Domain Expertise: Industry-specific knowledge, compliance, regulations\n",
      "        - Role Types: Job titles, seniority levels, functional areas\n",
      "        - Certifications: Professional credentials, vendor certifications\n",
      "        - Soft Skills: Leadership, communication, project management\n",
      "\n",
      "        #### Boolean Search Format Examples:\n",
      "        - \"bioinformatics\" AND (\"machine learning\" OR \"self-supervised\" OR \"AI\") AND (Python OR R), \n",
      "        - \"precision medicine\" AND (\"tissue microarray\" OR \"image analysis\" OR pathology) AND (self-supervised OR deep learning), \n",
      "        - \"genomics\" AND (\"variant calling\" OR alignment OR \"genomic data visualization\") AND (Python OR R), \n",
      "        - \"single cell\" OR \"spatial transcriptomics\" AND (analysis OR pipeline) AND (Snakemake OR Nextflow), \n",
      "        - (pharmaceutical OR drug testing), \"workflow management\" AND (Snakemake OR Nextflow) AND (Docker OR containerization), \n",
      "        - \"bioinformatics\" AND (HPC OR \"high performance computing\") AND (Python OR R), \n",
      "        - \"multi-omics\" AND (integration OR \"data analysis\") AND (R OR Python) \n",
      "\n",
      "        ### Exclude: Company names, location names, personal information\n",
      "\n",
      "        ## EXPERIENCE LEVEL (Precise Classification)\n",
      "        Calculate total professional work experience using these rules:\n",
      "        ### Counting Rules:\n",
      "        - Include: Full-time roles, significant part-time roles (20+ hours/week), consulting/freelance\n",
      "        - Exclude: Internships, fellowships, student work, volunteer work\n",
      "        - Overlapping roles: Count the primary role period only\n",
      "        - Career gaps: Subtract gaps longer than 6 months unless for education/family reasons\n",
      "\n",
      "        ### Classification Tiers:\n",
      "        - ENTRY: New graduates OR <2 years professional experience. Ignore the internship or fellowship experience.\n",
      "        - JUNIOR: 2-5 years of professional experience\n",
      "        - MID: 5-10 years of professional experience\n",
      "        - SENIOR: 10-15 years of professional experience\n",
      "        - LEAD: 15-20 years of professional experience\n",
      "        - EXECUTIVE: 20+ years of professional experience OR C-level/VP titles\n",
      "\n",
      "        ## TARGET COMPANY PROFILE\n",
      "        Analyze the candidate's background to identify ideal company characteristics:\n",
      "        - Company size: Startup (1-50), Scale-up (51-500), Mid-market (501-5000), Enterprise (5000+)\n",
      "        - Industry sectors: Based on previous experience and skills\n",
      "        - Company stage: Early-stage, growth-stage, mature, public company\n",
      "        - Culture indicators: Remote-first, innovation-focused, enterprise-focused, etc.\n",
      "        - Technology stack alignment: Companies using similar technologies\n",
      "\n",
      "        ## QUALITY ASSURANCE RULES\n",
      "        ### Accuracy Standards\n",
      "        - No hallucination: Only include information explicitly stated or clearly inferrable from the resume\n",
      "        - Prefer null values over guessed information when data is unclear\n",
      "        - Verify consistency between different resume sections\n",
      "        - Cross-reference dates to ensure timeline accuracy\n",
      "\n",
      "        ### Completeness Checks\n",
      "        - Ensure all required ResumeModel fields are populated\n",
      "        - Flag any critical missing information that might hurt job search effectiveness\n",
      "        - Validate that keywords align with actual experience and skills\n",
      "\n",
      "        ### Optimization Guidelines\n",
      "        - ATS-friendly formatting: Use standard terminology and keywords\n",
      "        - Recruiter-focused language: Emphasize measurable achievements and business impact\n",
      "        - Market-aware positioning: Position candidate competitively within their experience level\n",
      "        - Search-optimized content: Include terms commonly used in job postings for target roles\n",
      "\n",
      "        ## OUTPUT REQUIREMENTS\n",
      "        Format: Return ONLY the complete JSON object conforming to ResumeModel schema - no additional text, explanations, or formatting.\n",
      "        ### Validation: Ensure the output:\n",
      "        - Passes JSON schema validation\n",
      "        - Contains all required fields\n",
      "        - Uses appropriate data types\n",
      "        - Includes realistic and accurate information\n",
      "        - Optimizes for job search effectiveness\n",
      "\n",
      "        ### EXAMPLE PROCESSING APPROACH\n",
      "        - Extract: Parse resume sections systematically\n",
      "        - Analyze: Identify key themes, progressions, and strengths\n",
      "        - Synthesize: Create compelling summary highlighting unique value\n",
      "        - Optimize: Generate keywords and positioning for target market\n",
      "        - Structure: Format into clean ResumeModel JSON output\n",
      "        - Validate: Verify accuracy and completeness before returning\n",
      "\n",
      "        NOTE: Your success is measured by how effectively the generated profile helps the candidate land\n",
      "        \n",
      "Reading resume from:\n",
      " ../data/Saaniya Desai.pdf\n",
      "resume from function_tool:\n",
      " Saaniya Desai\n",
      "github.com/saaniyadesai, saaniya.desai@gmail.com\n",
      "E X P E R I E N C E \n",
      "May 2024 — Sep 2025 MSc Bioinformatics, The Hughes Group University of Glasgow\n",
      "• Project title: \"A Machine Learning Model for Virus Integration Site V alidation\"\n",
      "• Developed a machine learning model to validate viral integration sites in cancer genomes, enhancing \n",
      "detection accuracy for genomic research.\n",
      "• Analyzed large-scale viral integration datasets to uncover host-virus interaction trends with potential \n",
      "therapeutic relevance.\n",
      "May 2023 — Sep 2024 MSc Precision Medicine, The Le Quesne Group University of Glasgow\n",
      "• Project title: \"Artificial Intelligence in the Analysis of High-Resolution Mesothelioma Tissue Microarrays\"\n",
      "• T rained a self-supervised AI model to discover recurrent morphologies related to patient outcome\n",
      "• Applied advanced image analysis to support translational oncology research.\n",
      "Jan 2023 — May 2023 BSc Cell Biology Project, The Marston Lab University of Edinburgh \n",
      "• Project title: \"Characterizing Receptors for the Cohesin Loader Mis4/Ssl3 in Schizosaccharomyces pombe\"\n",
      "• Project involving integrating cell culturing, genome editing, PCR, western blotting, and SDS-PAGE.\n",
      "• Produced reproducible experimental workflows for ongoing genetics research projects.\n",
      "May 2022 — Aug 2022 ORISE Fellow, United States Food and Drug Administration Silver Spring, MD\n",
      "• Conducted biochemical analyses using IC and GC, LC/MS, and LC-MS/MS to support pharmaceutical \n",
      "quality evaluation.\n",
      "• Authored and presented research on nitrosamine impurity risk mitigation at the 2022 FDA Student \n",
      "Abstracts conference.\n",
      "• Delivered analytical findings to the Office of T esting and Research and Office of Pharmaceutical Quality, \n",
      "contributing to FDA regulatory assessments.\n",
      "Jul 2021 — Sep 2021 Research Intern, MassBiologics Mattapan, MA\n",
      "• Project title: “Generation of Episomally Stable Fluorescent MEXi-293 Cells”\n",
      "• Performed aseptic mammalian and bacterial cell culture (MEXi, CHO, E.coli), immunological assays, \n",
      "bioanalyzer runs, and antibody purification via Fed Batch.\n",
      "Jul 2020 — Sep 2020 Research Intern, Pine Biotech Online\n",
      "• Applied genomics, transcriptomics, and metagenomics pipelines to analyze raw sequencing datasets in \n",
      "oncology and precision medicine projects.\n",
      "• Professional training in bioinformatics workflow development and multi-omics data integration.\n",
      "Jul 2018 — Aug 2018 ESP Summer Intern, AstraZeneca W altham, MA\n",
      "• V erified covalent binding of T agrisso to Cys797 on EGFR through mass spectrometry-based analysis.\n",
      "• Investigated HER1, EGFR, and EED pathways for potential lung and breast cancer treatments.\n",
      "S K I L L S Technical: Python, R, SQL, Bash, Git, Docker, HPC Environments  \n",
      "Bioinformatics: V ariant Calling, Alignment T ools, Genomic Data Visualization, Single Cell & Spatial \n",
      "T ranscriptomics, Multi-Omics Integration, Genomic Databases(NCBI, UCSC T able Browser, COSMIC, etc.) \n",
      "AI/ML & Data Analysis: Machine Learning, Self-Supervised & Unsupervised Learning  \n",
      "W orkflow: W orkflow Management (Snakemake, Nextflow), Containerization, V ersion Control\n",
      "P U B L I C AT I O N S \n",
      "2025  “A headspace GC–MS method to quantify nitrosamine impurities and precursors in drug products: Method \n",
      "validation and product testing,” Biomedical Chromatography\n",
      "2024 “Nitrosamine mitigation: NDMA impurity formation and its inhibition in metformin hydrochloride tablets,” \n",
      "International Journal of Pharmaceutics\n",
      "2023 “Bumetanide as a Model NDSRI Substrate: N-nitrosobumetanide Impurity Formation and its Inhibition in \n",
      "Bumetanide T ablets,” Journal of Pharmaceutical SciencesE D U C AT I O N \n",
      "Sep 2024 — Sep 2025 MSc Bioinformatics University of Glasgow\n",
      "Sep 2023 — Sep 2024 MSc Precision Medicine (With Cancer Specialism) University of Glasgow\n",
      "Graduated with Distinction\n",
      "Sep 2019 — May 2023 BSc (Hons) Biological Sciences (Cell Biology) University of Edinburgh\n",
      "Grdauated with Upper Second Class Honors\n",
      "profiled_resume:\n",
      " name='Saaniya Desai' email='saaniya.desai@gmail.com' phone=None linkedin=None github='https://github.com/saaniyadesai' skills=['Python', 'R', 'SQL', 'Bash', 'Git', 'Docker', 'HPC environments', 'Variant calling', 'Alignment tools', 'Genomic data visualization', 'Single-cell transcriptomics', 'Spatial transcriptomics', 'Multi-omics integration', 'Machine learning', 'Self-supervised learning', 'Unsupervised learning', 'Snakemake', 'Nextflow', 'LC-MS / GC-MS / IC', 'Mass spectrometry'] experiences=[Experience(title='MSc Bioinformatics Project — The Hughes Group', company='University of Glasgow', start_date='May 2024', end_date='Sep 2025', description='Developed a machine learning model to validate viral integration sites in cancer genomes; analyzed large-scale viral integration datasets to identify host–virus interaction trends with potential therapeutic relevance.'), Experience(title='MSc Precision Medicine Project — The Le Quesne Group', company='University of Glasgow', start_date='May 2023', end_date='Sep 2024', description='Trained a self-supervised AI model to discover recurrent morphologies in high-resolution mesothelioma tissue microarrays; applied advanced image analysis methods to support translational oncology research.'), Experience(title='BSc Cell Biology Project — The Marston Lab', company='University of Edinburgh', start_date='Jan 2023', end_date='May 2023', description='Characterized receptors for the cohesin loader Mis4/Ssl3 in Schizosaccharomyces pombe using cell culture, genome editing, PCR, western blotting, and SDS-PAGE; produced reproducible experimental workflows.'), Experience(title='ORISE Fellow', company='United States Food and Drug Administration (FDA)', start_date='May 2022', end_date='Aug 2022', description='Conducted biochemical analyses using IC, GC, LC-MS and LC-MS/MS to support pharmaceutical quality evaluation; authored and presented research on nitrosamine impurity risk mitigation to FDA offices and at the 2022 FDA Student Abstracts conference.'), Experience(title='Research Intern', company='MassBiologics', start_date='Jul 2021', end_date='Sep 2021', description='Generated episomally stable fluorescent MEXi-293 cells; performed aseptic mammalian and bacterial cell culture, immunoassays, bioanalyzer runs, and antibody purification via fed-batch processes.'), Experience(title='Research Intern — Bioinformatics', company='Pine Biotech (online)', start_date='Jul 2020', end_date='Sep 2020', description='Applied genomics, transcriptomics, and metagenomics pipelines to analyze raw sequencing datasets for oncology and precision medicine projects; trained in workflow development and multi-omics integration.'), Experience(title='ESP Summer Intern — Analytical Research', company='AstraZeneca', start_date='Jul 2018', end_date='Aug 2018', description='Performed mass spectrometry-based verification of covalent binding events and investigated signaling pathways relevant to oncology drug discovery.')] education=[Education(degree='MSc Bioinformatics', institution='University of Glasgow', start_date='Sep 2024', end_date='Sep 2025', description=\"Project: 'A Machine Learning Model for Virus Integration Site Validation' — focus on ML for genomics and large-scale viral integration data analysis.\"), Education(degree='MSc Precision Medicine (Cancer Specialism)', institution='University of Glasgow', start_date='Sep 2023', end_date='Sep 2024', description=\"Graduated with Distinction. Project: 'Artificial Intelligence in the Analysis of High-Resolution Mesothelioma Tissue Microarrays' — trained self-supervised models and applied image analysis for translational oncology.\"), Education(degree='BSc (Hons) Biological Sciences (Cell Biology)', institution='University of Edinburgh', start_date='Sep 2019', end_date='May 2023', description='Graduated with Upper Second Class Honors. Capstone project on cohesin loader receptors in S. pombe combining wet-lab and molecular techniques.')] projects=[Project(name='A Machine Learning Model for Virus Integration Site Validation', description='ML model developed to validate viral integration sites in cancer genomes and analyze host–virus interaction patterns.', link=None), Project(name='Artificial Intelligence in the Analysis of High-Resolution Mesothelioma Tissue Microarrays', description='Trained a self-supervised model to discover recurrent morphologies associated with patient outcomes; applied image analysis for translational research.', link=None), Project(name='Characterizing Receptors for the Cohesin Loader Mis4/Ssl3', description='Integrated cell culturing, genome editing, PCR, western blotting, and SDS-PAGE to characterize receptor biology in S. pombe.', link=None), Project(name='Generation of Episomally Stable Fluorescent MEXi-293 Cells', description='Cell line engineering and antibody purification workflows used for downstream biologics research.', link=None)] certifications=[] publications=[Publication(title='A headspace GC–MS method to quantify nitrosamine impurities and precursors in drug products: Method validation and product testing', publisher='Biomedical Chromatography', date='2025', link=None), Publication(title='Nitrosamine mitigation: NDMA impurity formation and its inhibition in metformin hydrochloride tablets', publisher='International Journal of Pharmaceutics', date='2024', link=None), Publication(title='Bumetanide as a Model NDSRI Substrate: N-nitrosobumetanide Impurity Formation and its Inhibition in Bumetanide Tablets', publisher='Journal of Pharmaceutical Sciences', date='2023', link=None)] patents=[] experience_level=<ExperienceLevel.ENTRY: 'entry'> summary='- Early-career computational biologist and bioinformatician with hands-on experience in ML-driven genomics and translational oncology research.\\n- Developed and validated a machine learning model for viral integration site detection in cancer genomes and analyzed large-scale genomic datasets for host–virus interaction insights.\\n- Trained self-supervised AI models on high-resolution tissue microarrays to extract morphology signatures linked to patient outcomes; strong translational focus.\\n- Practical wet-lab and analytical chemistry experience (IC, GC, LC-MS/MS) from FDA ORISE fellowship and industry internships, with multiple peer-reviewed publications.\\n- Skilled in Python, R, Snakemake/Nextflow workflows, containerization (Docker), HPC environments, variant calling, alignment tools, and multi-omics integration.\\n- Demonstrated ability to communicate results to regulatory and research stakeholders (presented at FDA Student Abstracts); strong reproducible-workflow mindset for computational and experimental research.' keywords=['\"bioinformatics\" AND (Python OR R)', '\"variant calling\" OR alignment OR \"genomic data visualization\"', '\"single cell\" OR \"spatial transcriptomics\" AND (analysis OR pipeline)', '\"multi-omics\" AND (integration OR analysis) AND (R OR Python)', '\"machine learning\" AND (self-supervised OR unsupervised) AND (Python OR R)', '\"workflow management\" AND (Snakemake OR Nextflow) AND (Docker OR containerization)', '\"high performance computing\" OR HPC AND (Python OR R)', '\"LC-MS\" OR \"GC-MS\" OR \"LC-MS/MS\" AND (analytical OR mass spectrometry)', '\"precision medicine\" AND (translational OR oncology)', '(variant OR VCF) AND (calling OR annotation) AND (Python OR R)', '\"genomic databases\" AND (NCBI OR UCSC OR COSMIC)', '\"image analysis\" AND (self-supervised OR deep learning) AND (pathology OR tissue)', '\"data integration\" AND (transcriptomics OR genomics OR metagenomics)', '\"containerization\" AND (Docker OR Singularity) AND (workflow OR reproducible)', '(Snakemake OR Nextflow) AND (pipeline OR workflow)', '\"research\" AND (FDA OR regulatory) AND (analytical OR method validation)', '\"mass spectrometry\" AND (method validation OR impurity analysis)', '\"translational oncology\" AND (tissue microarray OR histopathology)'] target_company_profile='Best fit: small-to-mid biotech or computational biology teams (startup to mid-market, 1–5000 employees) focused on translational oncology, precision medicine, or genomics diagnostics. Ideal stages: early-stage to growth-stage companies or research institutes/CROs with strong emphasis on reproducible computational pipelines, ML-driven biomarker discovery, and integration with wet-lab/analytical chemistry. Culture fit: research-focused, collaborative, translational, and remote-friendly or hybrid teams using Python/R, Snakemake/Nextflow, Docker, and HPC resources.'\n"
     ]
    }
   ],
   "source": [
    "profiled_resume = await ResumeProcessorAgent(\"Saaniya Desai\", MODEL).run_with_trace()\n",
    "\n",
    "print(f\"profiled_resume:\\n {profiled_resume}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19a90120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Saaniya Desai' email='saaniya.desai@gmail.com' phone=None linkedin=None github='https://github.com/saaniyadesai' skills=['Python', 'R', 'SQL', 'Bash', 'Git', 'Docker', 'HPC environments', 'Variant calling', 'Alignment tools', 'Genomic data visualization', 'Single-cell transcriptomics', 'Spatial transcriptomics', 'Multi-omics integration', 'Machine learning', 'Self-supervised learning', 'Unsupervised learning', 'Snakemake', 'Nextflow', 'LC-MS / GC-MS / IC', 'Mass spectrometry'] experiences=[Experience(title='MSc Bioinformatics Project — The Hughes Group', company='University of Glasgow', start_date='May 2024', end_date='Sep 2025', description='Developed a machine learning model to validate viral integration sites in cancer genomes; analyzed large-scale viral integration datasets to identify host–virus interaction trends with potential therapeutic relevance.'), Experience(title='MSc Precision Medicine Project — The Le Quesne Group', company='University of Glasgow', start_date='May 2023', end_date='Sep 2024', description='Trained a self-supervised AI model to discover recurrent morphologies in high-resolution mesothelioma tissue microarrays; applied advanced image analysis methods to support translational oncology research.'), Experience(title='BSc Cell Biology Project — The Marston Lab', company='University of Edinburgh', start_date='Jan 2023', end_date='May 2023', description='Characterized receptors for the cohesin loader Mis4/Ssl3 in Schizosaccharomyces pombe using cell culture, genome editing, PCR, western blotting, and SDS-PAGE; produced reproducible experimental workflows.'), Experience(title='ORISE Fellow', company='United States Food and Drug Administration (FDA)', start_date='May 2022', end_date='Aug 2022', description='Conducted biochemical analyses using IC, GC, LC-MS and LC-MS/MS to support pharmaceutical quality evaluation; authored and presented research on nitrosamine impurity risk mitigation to FDA offices and at the 2022 FDA Student Abstracts conference.'), Experience(title='Research Intern', company='MassBiologics', start_date='Jul 2021', end_date='Sep 2021', description='Generated episomally stable fluorescent MEXi-293 cells; performed aseptic mammalian and bacterial cell culture, immunoassays, bioanalyzer runs, and antibody purification via fed-batch processes.'), Experience(title='Research Intern — Bioinformatics', company='Pine Biotech (online)', start_date='Jul 2020', end_date='Sep 2020', description='Applied genomics, transcriptomics, and metagenomics pipelines to analyze raw sequencing datasets for oncology and precision medicine projects; trained in workflow development and multi-omics integration.'), Experience(title='ESP Summer Intern — Analytical Research', company='AstraZeneca', start_date='Jul 2018', end_date='Aug 2018', description='Performed mass spectrometry-based verification of covalent binding events and investigated signaling pathways relevant to oncology drug discovery.')] education=[Education(degree='MSc Bioinformatics', institution='University of Glasgow', start_date='Sep 2024', end_date='Sep 2025', description=\"Project: 'A Machine Learning Model for Virus Integration Site Validation' — focus on ML for genomics and large-scale viral integration data analysis.\"), Education(degree='MSc Precision Medicine (Cancer Specialism)', institution='University of Glasgow', start_date='Sep 2023', end_date='Sep 2024', description=\"Graduated with Distinction. Project: 'Artificial Intelligence in the Analysis of High-Resolution Mesothelioma Tissue Microarrays' — trained self-supervised models and applied image analysis for translational oncology.\"), Education(degree='BSc (Hons) Biological Sciences (Cell Biology)', institution='University of Edinburgh', start_date='Sep 2019', end_date='May 2023', description='Graduated with Upper Second Class Honors. Capstone project on cohesin loader receptors in S. pombe combining wet-lab and molecular techniques.')] projects=[Project(name='A Machine Learning Model for Virus Integration Site Validation', description='ML model developed to validate viral integration sites in cancer genomes and analyze host–virus interaction patterns.', link=None), Project(name='Artificial Intelligence in the Analysis of High-Resolution Mesothelioma Tissue Microarrays', description='Trained a self-supervised model to discover recurrent morphologies associated with patient outcomes; applied image analysis for translational research.', link=None), Project(name='Characterizing Receptors for the Cohesin Loader Mis4/Ssl3', description='Integrated cell culturing, genome editing, PCR, western blotting, and SDS-PAGE to characterize receptor biology in S. pombe.', link=None), Project(name='Generation of Episomally Stable Fluorescent MEXi-293 Cells', description='Cell line engineering and antibody purification workflows used for downstream biologics research.', link=None)] certifications=[] publications=[Publication(title='A headspace GC–MS method to quantify nitrosamine impurities and precursors in drug products: Method validation and product testing', publisher='Biomedical Chromatography', date='2025', link=None), Publication(title='Nitrosamine mitigation: NDMA impurity formation and its inhibition in metformin hydrochloride tablets', publisher='International Journal of Pharmaceutics', date='2024', link=None), Publication(title='Bumetanide as a Model NDSRI Substrate: N-nitrosobumetanide Impurity Formation and its Inhibition in Bumetanide Tablets', publisher='Journal of Pharmaceutical Sciences', date='2023', link=None)] patents=[] experience_level=<ExperienceLevel.ENTRY: 'entry'> summary='- Early-career computational biologist and bioinformatician with hands-on experience in ML-driven genomics and translational oncology research.\\n- Developed and validated a machine learning model for viral integration site detection in cancer genomes and analyzed large-scale genomic datasets for host–virus interaction insights.\\n- Trained self-supervised AI models on high-resolution tissue microarrays to extract morphology signatures linked to patient outcomes; strong translational focus.\\n- Practical wet-lab and analytical chemistry experience (IC, GC, LC-MS/MS) from FDA ORISE fellowship and industry internships, with multiple peer-reviewed publications.\\n- Skilled in Python, R, Snakemake/Nextflow workflows, containerization (Docker), HPC environments, variant calling, alignment tools, and multi-omics integration.\\n- Demonstrated ability to communicate results to regulatory and research stakeholders (presented at FDA Student Abstracts); strong reproducible-workflow mindset for computational and experimental research.' keywords=['\"bioinformatics\" AND (Python OR R)', '\"variant calling\" OR alignment OR \"genomic data visualization\"', '\"single cell\" OR \"spatial transcriptomics\" AND (analysis OR pipeline)', '\"multi-omics\" AND (integration OR analysis) AND (R OR Python)', '\"machine learning\" AND (self-supervised OR unsupervised) AND (Python OR R)', '\"workflow management\" AND (Snakemake OR Nextflow) AND (Docker OR containerization)', '\"high performance computing\" OR HPC AND (Python OR R)', '\"LC-MS\" OR \"GC-MS\" OR \"LC-MS/MS\" AND (analytical OR mass spectrometry)', '\"precision medicine\" AND (translational OR oncology)', '(variant OR VCF) AND (calling OR annotation) AND (Python OR R)', '\"genomic databases\" AND (NCBI OR UCSC OR COSMIC)', '\"image analysis\" AND (self-supervised OR deep learning) AND (pathology OR tissue)', '\"data integration\" AND (transcriptomics OR genomics OR metagenomics)', '\"containerization\" AND (Docker OR Singularity) AND (workflow OR reproducible)', '(Snakemake OR Nextflow) AND (pipeline OR workflow)', '\"research\" AND (FDA OR regulatory) AND (analytical OR method validation)', '\"mass spectrometry\" AND (method validation OR impurity analysis)', '\"translational oncology\" AND (tissue microarray OR histopathology)'] target_company_profile='Best fit: small-to-mid biotech or computational biology teams (startup to mid-market, 1–5000 employees) focused on translational oncology, precision medicine, or genomics diagnostics. Ideal stages: early-stage to growth-stage companies or research institutes/CROs with strong emphasis on reproducible computational pipelines, ML-driven biomarker discovery, and integration with wet-lab/analytical chemistry. Culture fit: research-focused, collaborative, translational, and remote-friendly or hybrid teams using Python/R, Snakemake/Nextflow, Docker, and HPC resources.'\n"
     ]
    }
   ],
   "source": [
    "print(profiled_resume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d2a1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobSearcherAgent:\n",
    "    \"\"\"\n",
    "    Agent to search for jobs based on the job profile.\n",
    "    @param job_profile: ResumeModel - The job profile of the candidate\n",
    "    @param model: str - The model to use for the agent\n",
    "    \"\"\"\n",
    "    def __init__(self, job_profile: ResumeModel, model: str = MODEL):\n",
    "        self.job_profile = job_profile\n",
    "        self.model = model\n",
    "        self.brave_env = {\"BRAVE_API_KEY\": os.getenv(\"BRAVE_API_KEY\")}\n",
    "\n",
    "    def get_system_prompt(self) -> str:\n",
    "        return f\"\"\"\n",
    "        You are an expert Job Search Agent specializing in finding active job opportunities that perfectly match a candidate's profile. \n",
    "        Your mission is to conduct comprehensive job searches across multiple platforms and extract detailed, actionable information about each opportunity.\n",
    "        You are given a candidate's profile as a JSON object represented by the Pydantic model ResumeModel.\n",
    "        Your task is to search AS MANY job search sites as possible using specified MCP servers for searching the web and \n",
    "        return a list of job postings as a JSON object represented by the Pydantic model JobPostingList.\n",
    "\n",
    "        ## Core Responsibilities\n",
    "        - Using Candidate's Profile, find the list of top 10 companies that the candidate would be most interested in working for. Search the job postings on their career pages.\n",
    "        - Deep Job Search: Use all available MCP tools to search extensively across job boards, company websites, and professional platforms\n",
    "        - Active Job Validation: Verify that job postings are current, active, and accepting applications\n",
    "        - Detailed Information Extraction: Extract comprehensive job details including application URLs, requirements, and company information\n",
    "        - Profile Matching: Ensure all found opportunities align with the candidate's experience level, skills\n",
    "\n",
    "        ## Search Strategy \n",
    "        ### Primary Search Sources (Use MCP tools for each):\n",
    "        - Look for jobs in North America and Canada and United Kingdom\n",
    "        - Major Job Boards: LinkedIn, Indeed, Glassdoor, ZipRecruiter, Monster or similar job boards\n",
    "        - Tech-Specific Platforms: Stack Overflow Jobs, AngelList, Dice, GitHub Jobs or similar platforms\n",
    "        - Company Career Pages: Direct searches on target company websites. \n",
    "        - Professional Networks: Remote work platforms, industry-specific job boards\n",
    "        - Government/Public Sector: USAJobs, state government portals (if relevant)\n",
    "\n",
    "        ### Search Methodology:\n",
    "        - Keyword-Based Search: Use the candidate's summary for initial broad searches\n",
    "        - Title-Specific Search: Search for specific job titles mentioned in skills\n",
    "        - Company-Specific Search: Search directly on target companies' career pages\n",
    "        - Location-Based Search: Consider remote, hybrid, and location preferences\n",
    "        - Experience-Level Filtering: Filter by appropriate experience level\n",
    "\n",
    "        ## Job Filtering Rules:\n",
    "        - Experience Level Match: Only include jobs appropriate for candidate's experience level\n",
    "        - Skill Relevance: Prioritize jobs requiring 70%+ of candidate's skills\n",
    "        - Active Status: Only include jobs actively accepting applications\n",
    "        - Application Accessibility: Ensure application process is clearly accessible\n",
    "\n",
    "        ## Search Execution Instructions\n",
    "        ### Step 1: Broad Discovery Search\n",
    "        - Use MCP web search tools with candidate's primary keywords\n",
    "        - Search major job boards with experience level filters\n",
    "        - Cast a wide net initially to discover opportunities\n",
    "\n",
    "        ### Step 2: Targeted Company Search\n",
    "        - Search career pages of companies in target company profile\n",
    "        - Use company names + \"careers\" or \"jobs\" in search queries\n",
    "        - Look for new/recent postings on company websites\n",
    "\n",
    "        ### Step 3: Deep Information Extraction\n",
    "        - For each promising job found, fetch the complete job posting\n",
    "        - Navigate to application pages to verify they're active\n",
    "        - Extract all required information fields\n",
    "\n",
    "        ### Step 4: Verification & Validation\n",
    "        - Verify job posting dates are recent (within last 90 days)\n",
    "        - Confirm application links are functional\n",
    "        - Check for duplicate postings across platforms\n",
    "\n",
    "        ## Format: JobPostingList JSON Schema\n",
    "        - Return a structured JSON object containing:\n",
    "            - job_postings: List[JobPosting] - List of job postings \n",
    "                - List of JobPosting searched for the candidate's profile SCHEMA of JobPosting: {JobPosting.model_json_schema()}\n",
    "            -   JobPostingList JSON SCHEMA: {JobPostingList.model_json_schema()}\n",
    "            \n",
    "        RULES:\n",
    "        3, IMPORTANT: You must use the candidate's summary (enclosed in <SUMMARY>...</SUMMARY> tags) to filter the job postings relevant to the candidate.\n",
    "        4. IMPORTANT: You must use the candidate's experience level (enclosed in <EXPERIENCE_LEVEL>...</EXPERIENCE_LEVEL> tags) to filter the job postings relevant to the candidate.\n",
    "        5. Return only JSON object, no other text.\n",
    "\n",
    "        Your successis measured by the number of ACTIVE/OPEN jobs found and the quality of the jobs found.\n",
    "        \"\"\"\n",
    "\n",
    "    def get_user_prompt(self) -> str:\n",
    "        return f\"\"\"\n",
    "         Use the following information to search for the most relevant jobs relevant to the candidate's profile.\n",
    "        <SUMMARY>\n",
    "        {self.job_profile.summary}\n",
    "        </SUMMARY>\n",
    "        <EXPERIENCE_LEVEL>\n",
    "        {self.job_profile.experience_level}\n",
    "        </EXPERIENCE_LEVEL>\n",
    "        \"\"\"\n",
    "\n",
    "    async def get_mcp_server_params(self) -> list:\n",
    "        \"\"\"\n",
    "        This function is used to get the MCP server parameters.\n",
    "        @return: list - List of dictionaries containing the MCP server parameters\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\"command\": \"uvx\", \"args\": [\"mcp-server-fetch\"]}, \n",
    "            {\n",
    "                \"command\": \"npx\",\n",
    "                \"args\": [\"-y\", \"@modelcontextprotocol/server-brave-search\"],\n",
    "                \"env\": self.brave_env,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    async def get_job_searcher_agent(self, mcp_server_list: list) -> Agent:\n",
    "        \"\"\"\n",
    "        This function is used to create a job searcher agent that can be used to search for jobs based on the candidate's profile.\n",
    "        @return: Agent - The job searcher agent\n",
    "        \"\"\"\n",
    "        system_prompt = self.get_system_prompt()\n",
    "        print(f\"system_prompt:\\n {system_prompt}\")\n",
    "        \n",
    "        job_searcher_agent = Agent(\n",
    "            name=\"JobSearcherAgent\",\n",
    "            instructions=system_prompt,\n",
    "            mcp_servers=mcp_server_list,\n",
    "            model=MODEL,\n",
    "            output_type=JobPostingList\n",
    "        )\n",
    "\n",
    "        return job_searcher_agent\n",
    "    \n",
    "    async def search_jobs(self) -> JobPostingList:\n",
    "        \"\"\"\n",
    "        This function is used to search for jobs based on the candidate's profile.\n",
    "        @return: JobPostingList - The list of job postings\n",
    "        \"\"\"\n",
    "        job_postings = None\n",
    "\n",
    "        user_prompt = self.get_user_prompt()\n",
    "        print(f\"user_prompt:\\n {user_prompt}\")\n",
    "\n",
    "        mcp_server_params = await self.get_mcp_server_params()\n",
    "        mcp_server_list = []\n",
    "        async with AsyncExitStack() as stack:\n",
    "            mcp_server_list = [\n",
    "                await stack.enter_async_context(\n",
    "                    MCPServerStdio(params, client_session_timeout_seconds=120)\n",
    "                )\n",
    "                for params in mcp_server_params\n",
    "            ]\n",
    "\n",
    "            job_searcher_agent = await self.get_job_searcher_agent(mcp_server_list)\n",
    "            \n",
    "            trace_id = gen_trace_id()\n",
    "            print(f\"Trace@ https://platform.openai.com/api/traces/{trace_id}\")\n",
    "            with trace(trace_id) as tracer:\n",
    "                job_postings = await Runner.run(job_searcher_agent, user_prompt, max_turns=30)\n",
    "\n",
    "        return job_postings.final_output if job_postings else None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44fc38f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_prompt:\n",
      " \n",
      "         Use the following information to search for the most relevant jobs relevant to the candidate's profile.\n",
      "        <SUMMARY>\n",
      "        - Early-career computational biologist and bioinformatician with hands-on experience in ML-driven genomics and translational oncology research.\n",
      "- Developed and validated a machine learning model for viral integration site detection in cancer genomes and analyzed large-scale genomic datasets for host–virus interaction insights.\n",
      "- Trained self-supervised AI models on high-resolution tissue microarrays to extract morphology signatures linked to patient outcomes; strong translational focus.\n",
      "- Practical wet-lab and analytical chemistry experience (IC, GC, LC-MS/MS) from FDA ORISE fellowship and industry internships, with multiple peer-reviewed publications.\n",
      "- Skilled in Python, R, Snakemake/Nextflow workflows, containerization (Docker), HPC environments, variant calling, alignment tools, and multi-omics integration.\n",
      "- Demonstrated ability to communicate results to regulatory and research stakeholders (presented at FDA Student Abstracts); strong reproducible-workflow mindset for computational and experimental research.\n",
      "        </SUMMARY>\n",
      "        <EXPERIENCE_LEVEL>\n",
      "        ExperienceLevel.ENTRY\n",
      "        </EXPERIENCE_LEVEL>\n",
      "        \n",
      "system_prompt:\n",
      " \n",
      "        You are an expert Job Search Agent specializing in finding active job opportunities that perfectly match a candidate's profile. \n",
      "        Your mission is to conduct comprehensive job searches across multiple platforms and extract detailed, actionable information about each opportunity.\n",
      "        You are given a candidate's profile as a JSON object represented by the Pydantic model ResumeModel.\n",
      "        Your task is to search AS MANY job search sites as possible using specified MCP servers for searching the web and \n",
      "        return a list of job postings as a JSON object represented by the Pydantic model JobPostingList.\n",
      "\n",
      "        ## Core Responsibilities\n",
      "        - Using Candidate's Profile, find the list of top 10 companies that the candidate would be most interested in working for. Search the job postings on their career pages.\n",
      "        - Deep Job Search: Use all available MCP tools to search extensively across job boards, company websites, and professional platforms\n",
      "        - Active Job Validation: Verify that job postings are current, active, and accepting applications\n",
      "        - Detailed Information Extraction: Extract comprehensive job details including application URLs, requirements, and company information\n",
      "        - Profile Matching: Ensure all found opportunities align with the candidate's experience level, skills\n",
      "\n",
      "        ## Search Strategy \n",
      "        ### Primary Search Sources (Use MCP tools for each):\n",
      "        - Look for jobs in North America and Canada and United Kingdom\n",
      "        - Major Job Boards: LinkedIn, Indeed, Glassdoor, ZipRecruiter, Monster or similar job boards\n",
      "        - Tech-Specific Platforms: Stack Overflow Jobs, AngelList, Dice, GitHub Jobs or similar platforms\n",
      "        - Company Career Pages: Direct searches on target company websites. \n",
      "        - Professional Networks: Remote work platforms, industry-specific job boards\n",
      "        - Government/Public Sector: USAJobs, state government portals (if relevant)\n",
      "\n",
      "        ### Search Methodology:\n",
      "        - Keyword-Based Search: Use the candidate's summary for initial broad searches\n",
      "        - Title-Specific Search: Search for specific job titles mentioned in skills\n",
      "        - Company-Specific Search: Search directly on target companies' career pages\n",
      "        - Location-Based Search: Consider remote, hybrid, and location preferences\n",
      "        - Experience-Level Filtering: Filter by appropriate experience level\n",
      "\n",
      "        ## Job Filtering Rules:\n",
      "        - Experience Level Match: Only include jobs appropriate for candidate's experience level\n",
      "        - Skill Relevance: Prioritize jobs requiring 70%+ of candidate's skills\n",
      "        - Active Status: Only include jobs actively accepting applications\n",
      "        - Application Accessibility: Ensure application process is clearly accessible\n",
      "\n",
      "        ## Search Execution Instructions\n",
      "        ### Step 1: Broad Discovery Search\n",
      "        - Use MCP web search tools with candidate's primary keywords\n",
      "        - Search major job boards with experience level filters\n",
      "        - Cast a wide net initially to discover opportunities\n",
      "\n",
      "        ### Step 2: Targeted Company Search\n",
      "        - Search career pages of companies in target company profile\n",
      "        - Use company names + \"careers\" or \"jobs\" in search queries\n",
      "        - Look for new/recent postings on company websites\n",
      "\n",
      "        ### Step 3: Deep Information Extraction\n",
      "        - For each promising job found, fetch the complete job posting\n",
      "        - Navigate to application pages to verify they're active\n",
      "        - Extract all required information fields\n",
      "\n",
      "        ### Step 4: Verification & Validation\n",
      "        - Verify job posting dates are recent (within last 90 days)\n",
      "        - Confirm application links are functional\n",
      "        - Check for duplicate postings across platforms\n",
      "\n",
      "        ## Format: JobPostingList JSON Schema\n",
      "        - Return a structured JSON object containing:\n",
      "            - job_postings: List[JobPosting] - List of job postings \n",
      "                - List of JobPosting searched for the candidate's profile SCHEMA of JobPosting: {'properties': {'title': {'description': 'EXACT Job title or position held', 'title': 'Title', 'type': 'string'}, 'company': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'EXACT Name of the company or organization', 'title': 'Company'}, 'location': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'EXACT Location of the job posting', 'title': 'Location'}, 'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Brief description of the job posting', 'title': 'Description'}, 'link': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'URL or link to the job posting. (CRITICAL - this must be the actual job description page)', 'title': 'Link'}, 'apply_link': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'URL or link to the job application. (CRITICAL - this must be the actual application link)', 'title': 'Apply Link'}, 'posted_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Date the job posting was posted', 'title': 'Posted Date'}, 'application_deadline': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Date the job application deadline', 'title': 'Application Deadline'}, 'hiring_manager_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Name of the hiring manager', 'title': 'Hiring Manager Name'}, 'keywords': {'description': 'List of relevant keywords for the job posting', 'items': {'type': 'string'}, 'title': 'Keywords', 'type': 'array'}, 'rating': {'anyOf': [{'type': 'number'}, {'type': 'null'}], 'default': None, 'description': \"Rating of the job posting from 0 to 10 based on the candidate's profile\", 'title': 'Rating'}}, 'required': ['title'], 'title': 'JobPosting', 'type': 'object'}\n",
      "            -   JobPostingList JSON SCHEMA: {'$defs': {'JobPosting': {'properties': {'title': {'description': 'EXACT Job title or position held', 'title': 'Title', 'type': 'string'}, 'company': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'EXACT Name of the company or organization', 'title': 'Company'}, 'location': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'EXACT Location of the job posting', 'title': 'Location'}, 'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Brief description of the job posting', 'title': 'Description'}, 'link': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'URL or link to the job posting. (CRITICAL - this must be the actual job description page)', 'title': 'Link'}, 'apply_link': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'URL or link to the job application. (CRITICAL - this must be the actual application link)', 'title': 'Apply Link'}, 'posted_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Date the job posting was posted', 'title': 'Posted Date'}, 'application_deadline': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Date the job application deadline', 'title': 'Application Deadline'}, 'hiring_manager_name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Name of the hiring manager', 'title': 'Hiring Manager Name'}, 'keywords': {'description': 'List of relevant keywords for the job posting', 'items': {'type': 'string'}, 'title': 'Keywords', 'type': 'array'}, 'rating': {'anyOf': [{'type': 'number'}, {'type': 'null'}], 'default': None, 'description': \"Rating of the job posting from 0 to 10 based on the candidate's profile\", 'title': 'Rating'}}, 'required': ['title'], 'title': 'JobPosting', 'type': 'object'}}, 'properties': {'job_postings': {'description': 'List of job postings', 'items': {'$ref': '#/$defs/JobPosting'}, 'title': 'Job Postings', 'type': 'array'}}, 'title': 'JobPostingList', 'type': 'object'}\n",
      "\n",
      "        RULES:\n",
      "        3, IMPORTANT: You must use the candidate's summary (enclosed in <SUMMARY>...</SUMMARY> tags) to filter the job postings relevant to the candidate.\n",
      "        4. IMPORTANT: You must use the candidate's experience level (enclosed in <EXPERIENCE_LEVEL>...</EXPERIENCE_LEVEL> tags) to filter the job postings relevant to the candidate.\n",
      "        5. Return only JSON object, no other text.\n",
      "\n",
      "        Your successis measured by the number of ACTIVE/OPEN jobs found and the quality of the jobs found.\n",
      "        \n",
      "Trace@ https://platform.openai.com/api/traces/trace_563892169f434a4980ecff1369d39bd0\n",
      "job_postings:\n",
      " job_postings=[JobPosting(title='Bioinformatics Scientist I', company='Tempus', location='Chicago, IL / Remote (US)', description='Entry-level bioinformatics role focusing on cancer genomics pipelines, variant calling, and ML-driven analyses to support translational oncology projects.', link='https://www.tempus.com/careers/', apply_link='https://www.tempus.com/careers/', posted_date=None, application_deadline=None, hiring_manager_name=None, keywords=['cancer genomics', 'variant calling', 'Python', 'R', 'Snakemake', 'Docker', 'machine learning', 'translational oncology', 'HPC'], rating=9.0), JobPosting(title='Associate Bioinformatics Scientist', company='Guardant Health', location='Redwood City, CA / Hybrid', description='Early-career bioinformatics role supporting ctDNA/oncology assay analysis and development of reproducible pipelines for clinical genomics.', link='https://www.guardanthealth.com/careers/', apply_link='https://www.guardanthealth.com/careers/', posted_date=None, application_deadline=None, hiring_manager_name=None, keywords=['clinical genomics', 'ctDNA', 'variant calling', 'Python', 'R', 'workflow automation', 'Docker', 'HPC', 'oncology'], rating=9.0), JobPosting(title='Bioinformatics Analyst (Early Career)', company='Illumina', location='San Diego, CA / Hybrid', description='Entry-level position working on NGS data analysis, pipeline development, and algorithm validation for genomics products; strong emphasis on reproducible workflows.', link='https://www.illumina.com/company/careers.html', apply_link='https://www.illumina.com/company/careers.html', posted_date=None, application_deadline=None, hiring_manager_name=None, keywords=['NGS', 'alignment', 'variant calling', 'Python', 'R', 'Nextflow', 'Snakemake', 'Docker', 'translational research'], rating=8.0), JobPosting(title='Bioinformatics Scientist - Early Career', company='Foundation Medicine (Roche)', location='Cambridge, MA / Hybrid', description='Role focused on cancer genomic profiling, assay analytics, and development of computational methods to support clinical decision-making.', link='https://www.foundationmedicine.com/careers', apply_link='https://www.foundationmedicine.com/careers', posted_date=None, application_deadline=None, hiring_manager_name=None, keywords=['cancer genomics', 'clinical assays', 'Python', 'R', 'variant interpretation', 'machine learning', 'Docker', 'HPC'], rating=9.0), JobPosting(title='Junior Computational Biologist', company='Broad Institute', location='Cambridge, MA', description='Entry-level computational biology role on projects integrating multi-omics and imaging data for translational cancer research; ideal for candidates with ML + genomics experience.', link='https://www.broadinstitute.org/careers', apply_link='https://www.broadinstitute.org/careers', posted_date=None, application_deadline=None, hiring_manager_name=None, keywords=['multi-omics', 'machine learning', 'computational pathology', 'image analysis', 'Python', 'R', 'Snakemake', 'Docker'], rating=9.0), JobPosting(title='Bioinformatics Research Assistant - Computational Genomics', company='Fred Hutchinson Cancer Center', location='Seattle, WA', description='Early-career position supporting oncology genomics projects, pipeline development, and data analysis for translational studies.', link='https://www.fredhutch.org/en/careers.html', apply_link='https://www.fredhutch.org/en/careers.html', posted_date=None, application_deadline=None, hiring_manager_name=None, keywords=['oncology', 'genomics', 'variant calling', 'Python', 'R', 'reproducible workflows', 'HPC', 'translational research'], rating=8.0), JobPosting(title='Bioinformatics Scientist I (Early Career)', company='Genentech', location='South San Francisco, CA', description='Entry-level computational role supporting biomarker discovery and genomic data analysis in oncology drug programs; collaboration with wet-lab and clinical teams.', link='https://www.gene.com/careers', apply_link='https://www.gene.com/careers', posted_date=None, application_deadline=None, hiring_manager_name=None, keywords=['biomarker discovery', 'oncology', 'multi-omics', 'Python', 'R', 'machine learning', 'Docker', 'HPC'], rating=9.0), JobPosting(title='Early-Career Bioinformatics Engineer', company='Invitae', location='San Francisco, CA / Remote (US)', description='Role focused on clinical genomics engineering, scalable pipelines, variant interpretation, and integration of computational tools into diagnostic workflows.', link='https://www.invitae.com/en/about/careers/', apply_link='https://www.invitae.com/en/about/careers/', posted_date=None, application_deadline=None, hiring_manager_name=None, keywords=['clinical genomics', 'diagnostics', 'pipeline engineering', 'Python', 'Snakemake', 'Docker', 'variant calling', 'regulatory communication'], rating=8.0), JobPosting(title='Junior Bioinformatics Scientist', company='Adaptive Biotechnologies', location='Seattle, WA / Hybrid', description='Entry-level role applying computational methods to immune profiling and oncology biomarker projects; includes ML-based analyses and pipeline development.', link='https://www.adaptivebiotech.com/careers/', apply_link='https://www.adaptivebiotech.com/careers/', posted_date=None, application_deadline=None, hiring_manager_name=None, keywords=['immune profiling', 'oncology', 'machine learning', 'Python', 'R', 'workflow automation', 'Docker', 'HPC'], rating=8.0), JobPosting(title='Computational Biologist - Early Career', company='Sema4', location='Stamford, CT / Remote (US)', description='Entry-level computational biology position supporting translational genomics, ML model development, and multi-omics integration for clinical research programs.', link='https://sema4.com/careers/', apply_link='https://sema4.com/careers/', posted_date=None, application_deadline=None, hiring_manager_name=None, keywords=['translational genomics', 'multi-omics', 'machine learning', 'Python', 'R', 'clinical research', 'Snakemake', 'Docker'], rating=8.0)]\n"
     ]
    }
   ],
   "source": [
    "job_postings = await JobSearcherAgent(profiled_resume, MODEL).search_jobs()\n",
    "\n",
    "print(f\"job_postings:\\n {job_postings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e375e1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bioinformatics Scientist I\n",
      "Tempus\n",
      "https://www.tempus.com/careers/\n",
      "https://www.tempus.com/careers/\n",
      "--------------------------------\n",
      "\n",
      "Associate Bioinformatics Scientist\n",
      "Guardant Health\n",
      "https://www.guardanthealth.com/careers/\n",
      "https://www.guardanthealth.com/careers/\n",
      "--------------------------------\n",
      "\n",
      "Bioinformatics Analyst (Early Career)\n",
      "Illumina\n",
      "https://www.illumina.com/company/careers.html\n",
      "https://www.illumina.com/company/careers.html\n",
      "--------------------------------\n",
      "\n",
      "Bioinformatics Scientist - Early Career\n",
      "Foundation Medicine (Roche)\n",
      "https://www.foundationmedicine.com/careers\n",
      "https://www.foundationmedicine.com/careers\n",
      "--------------------------------\n",
      "\n",
      "Junior Computational Biologist\n",
      "Broad Institute\n",
      "https://www.broadinstitute.org/careers\n",
      "https://www.broadinstitute.org/careers\n",
      "--------------------------------\n",
      "\n",
      "Bioinformatics Research Assistant - Computational Genomics\n",
      "Fred Hutchinson Cancer Center\n",
      "https://www.fredhutch.org/en/careers.html\n",
      "https://www.fredhutch.org/en/careers.html\n",
      "--------------------------------\n",
      "\n",
      "Bioinformatics Scientist I (Early Career)\n",
      "Genentech\n",
      "https://www.gene.com/careers\n",
      "https://www.gene.com/careers\n",
      "--------------------------------\n",
      "\n",
      "Early-Career Bioinformatics Engineer\n",
      "Invitae\n",
      "https://www.invitae.com/en/about/careers/\n",
      "https://www.invitae.com/en/about/careers/\n",
      "--------------------------------\n",
      "\n",
      "Junior Bioinformatics Scientist\n",
      "Adaptive Biotechnologies\n",
      "https://www.adaptivebiotech.com/careers/\n",
      "https://www.adaptivebiotech.com/careers/\n",
      "--------------------------------\n",
      "\n",
      "Computational Biologist - Early Career\n",
      "Sema4\n",
      "https://sema4.com/careers/\n",
      "https://sema4.com/careers/\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for job_posting in job_postings.job_postings:\n",
    "    print(job_posting.title)\n",
    "    print(job_posting.company)\n",
    "    print(job_posting.link)\n",
    "    print(job_posting.apply_link)\n",
    "    print(\"--------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bcb1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
